---
title: Airflow Connector | Collate Workflow Orchestration
description: Connect Apache Airflow to Collate effortlessly. Complete setup guide, configuration steps, and pipeline metadata extraction for seamless data lineage tracking.
slug: /connectors/pipeline/airflow
sidebarTitle: Overview
---
import { ConnectorDetailsHeader } from '/snippets/components/ConnectorDetailsHeader/ConnectorDetailsHeader.jsx'
import ConfigureIngestion from '/snippets/connectors/pipeline/configure-ingestion.mdx'
import TestConnection from '/snippets/connectors/test-connection.mdx'
import IngestionScheduleAndDeploy from '/snippets/connectors/ingestion-schedule-and-deploy.mdx'
import { MetadataIngestionUi } from '/snippets/components/MetadataIngestionUi.jsx'

<ConnectorDetailsHeader
icon='/public/images/connectors/airflow.webp'
name="Airflow"
stage="PROD"
availableFeatures={["Pipelines", "Pipeline Status", "Lineage", "Owners", "Usage"]}
unavailableFeatures={["Tags"]} />
In this section, we provide guides and references to use the Airflow connector.
Configure and schedule Airflow metadata workflow from the OpenMetadata UI:
- [Troubleshooting](/connectors/pipeline/airflow/troubleshooting)
<IngestionModeTiles yamlPath="/snippets/connectors/pipeline/airflow/yaml" />
<CardGroup cols={2}>
<Card title="MWAA" href="/deployment/ingestion/external/mwaa">
Run the ingestion framework externally!
</Card>
<Card title="GCP Composer" href="/deployment/ingestion/external/gcp-composer">
Run the ingestion from GCP Composer.
</Card>
</CardGroup>
## Requirements
<Tip>
We only support officially supported Airflow versions.
You can check the version list [here](https://airflow.apache.org/docs/apache-airflow/stable/installation/supported-versions.html).
</Tip>
## Metadata Ingestion
<MetadataIngestionUi connector={"Airflow"} selectServicePath={"/public/images/connectors/airflow/select-service.png"} addNewServicePath={"/public/images/connectors/airflow/add-new-service.png"} serviceConnectionPath={"/public/images/connectors/airflow/service-connection.png"} />
# Connection Details
<Steps>
<Step title="Connection Details">
<Tip>
When using a **Hybrid Ingestion Runner**, any sensitive credential fields—such as passwords, API keys, or private keys—must reference secrets using the following format:
```
password: secret:/my/database/password
```
This applies **only to fields marked as secrets** in the connection form (these typically mask input and show a visibility toggle icon).
For a complete guide on managing secrets in hybrid setups, see the [Hybrid Ingestion Runner Secret Management Guide](https://docs.getcollate.io/getting-started/day-1/hybrid-saas/hybrid-ingestion-runner#3.-manage-secrets-securely).
</Tip>
- **Host and Port**: URL to the Airflow instance.
- **Number of Status**: Number of status we want to look back to in every ingestion (e.g., Past executions from a DAG).
- **Connection**: Airflow metadata database connection. See these [docs](https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html)
  for supported backends.
In terms of `connection` we support the following selections:
- `backend`: Should not be used from the UI. This is only applicable when ingesting Airflow metadata locally
    by running the ingestion from a DAG. It will use the current Airflow SQLAlchemy connection to extract the data.
- `MySQL`, `Postgres`, and `SQLite`: Pass the required credentials to reach out each of these services. We
    will create a connection to the pointed database and read Airflow data from there.
</Step>
<TestConnection />
<ConfigureIngestion />
<IngestionScheduleAndDeploy />
</Steps>
