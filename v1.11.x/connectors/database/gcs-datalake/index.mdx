---
title: GCS Datalake Connector | Collate Cloud Storage Integration
description: Connect your Google Cloud Storage data lake to Collate with our comprehensive GCS connector guide. Setup instructions, configuration tips & best practices.
slug: /connectors/database/gcs-datalake
sidebarTitle: Overview
---
import { ConnectorDetailsHeader } from '/snippets/components/ConnectorDetailsHeader/ConnectorDetailsHeader.jsx'
import TestConnection from '/snippets/connectors/test-connection.mdx'
import IngestionScheduleAndDeploy from '/snippets/connectors/ingestion-schedule-and-deploy.mdx'
import ConfigureIngestion from '/snippets/connectors/database/configure-ingestion.mdx'
import AdvancedConfiguration from '/snippets/connectors/database/advanced-configuration.mdx'
import { MetadataIngestionUi } from '/snippets/components/MetadataIngestionUi.jsx'
import Related from '/snippets/connectors/database/related.mdx'

<ConnectorDetailsHeader
icon='/public/images/connectors/gcs.webp'
name="GCS Datalake"
stage="PROD"
availableFeatures={["Metadata", "Data Profiler", "Data Quality", "Sample Data", "Auto-Classification"]}
unavailableFeatures={["Query Usage", "Lineage", "Column-level Lineage", "Owners", "dbt", "Tags", "Stored Procedures"]} />
In this section, we provide guides and references to use the GCS Datalake connector.
Configure and schedule GCS Datalake metadata and profiler workflows from the OpenMetadata UI:
- [Requirements](#requirements)
- [Metadata Ingestion](#metadata-ingestion)
- [Data Profiler](/how-to-guides/data-quality-observability/profiler/profiler-workflow)
- [Data Quality](/how-to-guides/data-quality-observability/quality)
- [Troubleshooting](/connectors/database/gcs-datalake/troubleshooting)
## Requirements
<Tip>
The GCS Datalake connector supports extracting metadata from file types `JSON`, `CSV`, `TSV` & `Parquet`.
</Tip>
## Metadata Ingestion
<MetadataIngestionUi connector={"GCS Datalake"} selectServicePath={"/public/images/connectors/datalake/select-service.png"} addNewServicePath={"/public/images/connectors/datalake/add-new-service.png"} serviceConnectionPath={"/public/images/connectors/datalake/service-connection.png"} />
## Connection Details
<Steps>
<Step title="Connection Details for GCS">
- **Bucket Name**: A bucket name in DataLake is a unique identifier used to organize and store data objects.
  It's similar to a folder name, but it's used for object storage rather than file storage.
- **Prefix**: The prefix of a data source in datalake refers to the first part of the data path that identifies the source or origin of the data. It's used to organize and categorize data within the datalake, and can help users easily locate and access the data they need.
**GCS Credentials**
We support two ways of authenticating to GCS:
1. Passing the raw credential values provided by BigQuery. This requires us to provide the following information, all provided by BigQuery:
   1. Credentials type, e.g. `service_account`.
   2. Project ID
   3. Private Key ID
   4. Private Key
   5. Client Email
   6. Client ID
   7. Auth URI, [https://accounts.google.com/o/oauth2/auth](https://accounts.google.com/o/oauth2/auth) by default
   8. Token URI, **https://oauth2.googleapis.com/token** by default
   9. Authentication Provider X509 Certificate URL, [https://www.googleapis.com/oauth2/v1/certs](https://www.googleapis.com/oauth2/v1/certs) by default
   10. Client X509 Certificate URL
</Step>
<AdvancedConfiguration />
<TestConnection />
<ConfigureIngestion />
<IngestionScheduleAndDeploy />
</Steps>
<Related />
