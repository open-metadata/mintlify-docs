---
title: Kafka Connector | Collate Messaging Integration
description: Connect Kafka to Collate effortlessly with our comprehensive connector guide. Set up messaging metadata ingestion, configuration, and monitoring in minutes.
slug: /connectors/messaging/kafka
sidebarTitle: Overview
---
import ConfigureIngestion from '/snippets/connectors/messaging/configure-ingestion.mdx'
import { ConnectorDetailsHeader } from '/snippets/components/ConnectorDetailsHeader/ConnectorDetailsHeader.jsx'
import TestConnection from '/snippets/connectors/test-connection.mdx'
import IngestionScheduleAndDeploy from '/snippets/connectors/ingestion-schedule-and-deploy.mdx'
import { MetadataIngestionUi } from '/snippets/components/MetadataIngestionUi.jsx'

<ConnectorDetailsHeader
icon='/public/images/connectors/kafka.webp'
name="Kafka"
stage="PROD"
availableFeatures={["Topics", "Sample Data"]}
unavailableFeatures={[]} />
In this section, we provide guides and references to use the Kafka connector.
Configure and schedule Kafka metadata and profiler workflows from the OpenMetadata UI:
- [Requirements](#requirements)
- [Metadata Ingestion](#metadata-ingestion)
- [Enable Security](#securing-kafka-connection-with-ssl-in-openmetadata)
- [Troubleshooting](/connectors/messaging/kafka/troubleshooting)
## Requirements
Connecting to Kafka does not require any previous configuration.
The ingestion of the Kafka topics' schema is done separately by configuring the **Schema Registry URL**. However, only the **Bootstrap Servers** information is mandatory. Below are the required Permissions for fetching the Metadata:-
- READ TOPIC
- DESCTIBE TOPIC
- READ CLUSTER
## Metadata Ingestion
<MetadataIngestionUi connector={"Kafka"} selectServicePath={"/public/images/connectors/kafka/select-service.png"} addNewServicePath={"/public/images/connectors/kafka/add-new-service.png"} serviceConnectionPath={"/public/images/connectors/kafka/service-connection.png"} />
# Connection Details
<Steps>
<Step title="Connection Details">
<Tip>
When using a **Hybrid Ingestion Runner**, any sensitive credential fields—such as passwords, API keys, or private keys—must reference secrets using the following format:
```
password: secret:/my/database/password
```
This applies **only to fields marked as secrets** in the connection form (these typically mask input and show a visibility toggle icon).
For a complete guide on managing secrets in hybrid setups, see the [Hybrid Ingestion Runner Secret Management Guide](https://docs.getcollate.io/getting-started/day-1/hybrid-saas/hybrid-ingestion-runner#3.-manage-secrets-securely).
</Tip>
- **Bootstrap Servers**: List of brokers as comma separated values of broker `host` or `host:port`. Example: `host1:9092,host2:9092`
- **Schema Registry URL**: URL of the Schema Registry used to ingest the schemas of the topics.
- **SASL Username**: SASL username for use with the PLAIN and SASL-SCRAM mechanisms.
- **SASL Password**: SASL password for use with the PLAIN and SASL-SCRAM mechanisms.
- **SASL Mechanism**: SASL mechanism to use for authentication.
- **Basic Auth User Info**: Schema Registry Client HTTP credentials in the form of `username:password`. By default, user info is extracted from the URL if present.
- **Consumer Config**: The accepted additional values for the consumer configuration can be found in the following [link](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.mdx).
If you are using Confluent kafka and SSL encryption is enabled you need to add `security.protocol` as key and `SASL_SSL` as value under Consumer Config
- **Schema Registry Config**: The accepted additional values for the Schema Registry configuration can be found in the following [link](https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#schemaregistryclient).
<Tip>
To ingest the topic schema `Schema Registry URL` must be passed
</Tip>
</Step>
<TestConnection />
<ConfigureIngestion />
<IngestionScheduleAndDeploy />
</Steps>
## Securing Kafka Connection with SSL in OpenMetadata
To establish secure connections between OpenMetadata and Kafka, navigate to the `Advanced Config` section. Here, you can provide the CA certificate used for SSL validation by specifying the `caCertificate`. Alternatively, if both client and server require mutual authentication, you'll need to use all three parameters: `ssl key`, `ssl cert`, and `caCertificate`. In this case, `ssl_cert` is used for the client’s SSL certificate, `ssl_key` for the private key associated with the SSL certificate, and `caCertificate` for the CA certificate to validate the server’s certificate.
  <img src="/public/images/connectors/ssl_kafka.png" alt="SSL Configuration" height="450px" />
